{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcf9ed4a"
      },
      "source": [
        "# Task\n",
        "Analyze the provided \"climate_nasa.csv\" dataset according to the instructions in the \"Climate Change Modeling.pdf\" document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43ad05fb"
      },
      "source": [
        "## Load and understand the data\n",
        "\n",
        "### Subtask:\n",
        "Load the data from the specified CSV file and perform initial exploratory data analysis to understand its structure and content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d21167bf"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the data from the specified CSV file into a pandas DataFrame and display the first few rows to understand its structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9dfd997a",
        "outputId": "9f1e0b4b-389d-4d0e-ba00-5b68d3dbc378"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('climate_nasa.csv')\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'climate_nasa.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1646951519.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'climate_nasa.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'climate_nasa.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09604b40"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the data failed because the file name was incorrect. The correct file name is 'climate_nasa.csv' based on the provided file list. I will retry loading the data with the correct file name and then proceed with displaying the first few rows, checking the column names and data types, displaying descriptive statistics for numerical columns, and checking for missing values as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "xWDZi2Es3MlV",
        "outputId": "86b8e892-173e-4a94-9d97-d4fa29cd6d12"
      },
      "source": [
        "df = pd.read_csv('/content/climate_nasa.csv')\n",
        "display(df.head())\n",
        "display(df.info())\n",
        "display(df.describe())\n",
        "display(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/climate_nasa.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4238065862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/climate_nasa.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/climate_nasa.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac66e2f0"
      },
      "source": [
        "## Preprocess the text data\n",
        "\n",
        "### Subtask:\n",
        "Clean and preprocess the text data for analysis, including tokenization, stop word removal, and lemmatization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d74b8a0"
      },
      "source": [
        "**Reasoning**:\n",
        "Handle missing values, clean the text, tokenize, remove stop words, lemmatize, and join tokens back into a string.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "913da7a5"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd # Import pandas\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.download('wordnet', quiet=True) # Added quiet=True\n",
        "    nltk.download('stopwords', quiet=True) # Added quiet=True\n",
        "    nltk.download('punkt', quiet=True) # Added quiet=True\n",
        "except LookupError:\n",
        "    print(\"NLTK data not found. Please ensure you have an internet connection to download.\")\n",
        "\n",
        "# Load the data (adding this step to ensure df is available)\n",
        "try:\n",
        "    df = pd.read_csv('/content/climate_nasa.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'climate_nasa.csv' not found. Please upload the file or check the path.\")\n",
        "    # Exit or handle the error appropriately if the file is not found\n",
        "    # For now, we will stop execution here if the file is not found\n",
        "    raise # Re-raise the exception\n",
        "\n",
        "\n",
        "# 1. Handle missing values\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# 2. Create 'clean_text' column with cleaning\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# 3. Tokenize\n",
        "df['clean_text'] = df['clean_text'].apply(nltk.word_tokenize)\n",
        "\n",
        "# 4. Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['clean_text'] = df['clean_text'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "\n",
        "# 5. Lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['clean_text'] = df['clean_text'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "# 6. Join tokens back into string\n",
        "df['clean_text'] = df['clean_text'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "display(df[['text', 'clean_text']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f28a2ca3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `nltk.downloader.DownloadError` does not exist. The correct exception to catch for missing NLTK data is `LookupError`. Also, the downloads should be done first before trying to use the resources.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHqk8zdq3SbX"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except LookupError:\n",
        "    print(\"NLTK data not found. Please ensure you have an internet connection to download.\")\n",
        "\n",
        "\n",
        "# 1. Handle missing values\n",
        "df['text'] = df['text'].fillna('')\n",
        "\n",
        "# 2. Create 'clean_text' column with cleaning\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# 3. Tokenize\n",
        "df['clean_text'] = df['clean_text'].apply(nltk.word_tokenize)\n",
        "\n",
        "# 4. Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['clean_text'] = df['clean_text'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "\n",
        "# 5. Lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['clean_text'] = df['clean_text'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "# 6. Join tokens back into string\n",
        "df['clean_text'] = df['clean_text'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "display(df[['text', 'clean_text']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "944d28fe"
      },
      "source": [
        "## Perform sentiment analysis\n",
        "\n",
        "### Subtask:\n",
        "Apply sentiment analysis techniques to the text data to understand the overall sentiment and its distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59dd5b01"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply sentiment analysis using VADER to the cleaned text data and create new columns for sentiment scores and labels as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a15b5566"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# 2. Instantiate the SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 3. Apply the analyzer to the clean_text column to get sentiment scores\n",
        "sentiment_scores = df['clean_text'].apply(analyzer.polarity_scores)\n",
        "\n",
        "# 4. Create new columns for negative, neutral, positive, and compound sentiment scores\n",
        "df['v_neg'] = sentiment_scores.apply(lambda x: x['neg'])\n",
        "df['v_neu'] = sentiment_scores.apply(lambda x: x['neu'])\n",
        "df['v_pos'] = sentiment_scores.apply(lambda x: x['pos'])\n",
        "df['v_compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
        "\n",
        "# 5. Create a categorical sentiment label column based on v_compound scores\n",
        "def get_sentiment_label(score):\n",
        "    if score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['vader_label'] = df['v_compound'].apply(get_sentiment_label)\n",
        "\n",
        "# 6. Display the head of the DataFrame with the new sentiment columns\n",
        "display(df[['v_neg', 'v_neu', 'v_pos', 'v_compound', 'vader_label']].head())\n",
        "\n",
        "# 7. Display the value counts of the vader_label column\n",
        "display(df['vader_label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4651a362"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the 'vader_lexicon' resource was not found. Need to download this NLTK resource before instantiating the SentimentIntensityAnalyzer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZtT0ErK3XxL"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download the vader_lexicon resource\n",
        "try:\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "except LookupError:\n",
        "    print(\"NLTK data 'vader_lexicon' not found. Please ensure you have an internet connection to download.\")\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# 2. Instantiate the SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 3. Apply the analyzer to the clean_text column to get sentiment scores\n",
        "sentiment_scores = df['clean_text'].apply(analyzer.polarity_scores)\n",
        "\n",
        "# 4. Create new columns for negative, neutral, positive, and compound sentiment scores\n",
        "df['v_neg'] = sentiment_scores.apply(lambda x: x['neg'])\n",
        "df['v_neu'] = sentiment_scores.apply(lambda x: x['neu'])\n",
        "df['v_pos'] = sentiment_scores.apply(lambda x: x['pos'])\n",
        "df['v_compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
        "\n",
        "# 5. Create a categorical sentiment label column based on v_compound scores\n",
        "def get_sentiment_label(score):\n",
        "    if score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['vader_label'] = df['v_compound'].apply(get_sentiment_label)\n",
        "\n",
        "# 6. Display the head of the DataFrame with the new sentiment columns\n",
        "display(df[['v_neg', 'v_neu', 'v_pos', 'v_compound', 'vader_label']].head())\n",
        "\n",
        "# 7. Display the value counts of the vader_label column\n",
        "display(df['vader_label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90bbcd0"
      },
      "source": [
        "## Perform topic modeling\n",
        "\n",
        "### Subtask:\n",
        "Apply topic modeling techniques to identify the main themes or topics present in the text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87f3ae96"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries and initialize CountVectorizer and LatentDirichletAllocation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43189f27"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "\n",
        "# Fit and transform the clean_text column\n",
        "dtm = vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "# Initialize LatentDirichletAllocation\n",
        "n_topics = 6\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "\n",
        "# Fit the LDA model\n",
        "lda.fit(dtm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d114acb"
      },
      "source": [
        "**Reasoning**:\n",
        "Get the topic distributions, create new columns for topic probabilities, determine the dominant topic, and display the top words for each topic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09fedd1e"
      },
      "source": [
        "# Get topic distributions for each document\n",
        "topic_distributions = lda.transform(dtm)\n",
        "\n",
        "# Create new columns for topic probabilities\n",
        "topic_columns = [f'topic_{i}' for i in range(n_topics)]\n",
        "df[topic_columns] = topic_distributions\n",
        "\n",
        "# Determine the dominant topic for each document\n",
        "df['dominant_topic'] = topic_distributions.argmax(axis=1)\n",
        "\n",
        "# Display the top words for each identified topic\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % topic_idx)\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 10  # Number of top words to display per topic\n",
        "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n",
        "\n",
        "# Display the head of the DataFrame with the new topic-related columns\n",
        "display(df[['clean_text'] + topic_columns + ['dominant_topic']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42690967"
      },
      "source": [
        "## Analyze trends over time\n",
        "\n",
        "### Subtask:\n",
        "Analyze how sentiment and topics change over time, if a date column is available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90b7940e"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the 'date' column to datetime objects, extract the month, calculate the average sentiment per month, count topic occurrences per month, and display the head of the resulting dataframes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ab1b7f4"
      },
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['month'] = df['date'].dt.to_period('M')\n",
        "\n",
        "sent_trend = df.groupby('month')['v_compound'].mean().reset_index()\n",
        "topic_trend = df.groupby(['month', 'dominant_topic']).size().reset_index(name='count')\n",
        "\n",
        "display(sent_trend.head())\n",
        "display(topic_trend.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8871c521"
      },
      "source": [
        "## Predict engagement\n",
        "\n",
        "### Subtask:\n",
        "Build a model to predict engagement metrics like likes or comments based on text features, if engagement data is available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0ce230"
      },
      "source": [
        "**Reasoning**:\n",
        "Prepare the data for modeling by selecting features and the target variable, handling missing values, and splitting the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb927f27"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Identify the target variable\n",
        "# We will predict 'likesCount'\n",
        "yr = df['likesCount']\n",
        "\n",
        "# 2. Select the features\n",
        "# Using 'text_len', 'v_compound', and topic probability columns\n",
        "# Need to create 'text_len' first\n",
        "df['text_len'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "Xr = df[['text_len', 'v_compound'] + [f'topic_{i}' for i in range(n_topics)]]\n",
        "\n",
        "# 3. Handle any remaining missing values in the selected features.\n",
        "# Check for NaNs in Xr\n",
        "print(\"Missing values in features before handling:\")\n",
        "display(Xr.isnull().sum())\n",
        "\n",
        "# There are no missing values in the selected features based on the output of the previous step.\n",
        "# If there were, we could use Xr.fillna(Xr.mean(), inplace=True) or other strategies.\n",
        "\n",
        "# 4. Split the data into training and testing sets.\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nShape of training features:\", X_train_r.shape)\n",
        "print(\"Shape of testing features:\", X_test_r.shape)\n",
        "print(\"Shape of training target:\", y_train_r.shape)\n",
        "print(\"Shape of testing target:\", y_test_r.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63c1be15"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a suitable regression model on the training data and evaluate its performance on the testing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce8ba9aa"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# 5. Choose a suitable regression model\n",
        "# Using Linear Regression as a starting point\n",
        "model = LinearRegression()\n",
        "\n",
        "# 6. Train the chosen model on the training data\n",
        "model.fit(X_train_r, y_train_r)\n",
        "\n",
        "# 7. Evaluate the model's performance on the testing data\n",
        "y_pred_r = model.predict(X_test_r)\n",
        "\n",
        "mae = mean_absolute_error(y_test_r, y_pred_r)\n",
        "mse = mean_squared_error(y_test_r, y_pred_r)\n",
        "r2 = r2_score(y_test_r, y_pred_r)\n",
        "\n",
        "print(\"Model Performance on Testing Data:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R-squared (R2): {r2:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c44a459d"
      },
      "source": [
        "## Incorporate instructions from the pdf\n",
        "\n",
        "### Subtask:\n",
        "Review the provided PDF document to understand any specific requirements, methodologies, or analyses requested for the project and implement them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73cd2d1a"
      },
      "source": [
        "## Summarize findings and visualize results\n",
        "\n",
        "### Subtask:\n",
        "Summarize the key findings from the analysis and create visualizations as needed to present the results effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72312f51"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate visualizations to summarize the analysis findings, including sentiment trend, topic trends, distribution of likes, and the relationship between text length and likes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15effa3a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert 'month' to string for plotting compatibility\n",
        "sent_trend['month_str'] = sent_trend['month'].astype(str)\n",
        "topic_trend['month_str'] = topic_trend['month'].astype(str)\n",
        "\n",
        "# 1. Line plot of monthly sentiment trend\n",
        "plt.figure(figsize=figsize)\n",
        "sns.lineplot(data=sent_trend, x='month_str', y='v_compound')\n",
        "plt.title('Monthly Sentiment Trend (Compound Score)')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Compound Sentiment Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Stacked bar chart of monthly topic trends\n",
        "# Convert 'dominant_topic' to string to ensure it's treated as categorical for stacking\n",
        "topic_trend['dominant_topic'] = topic_trend['dominant_topic'].astype(str)\n",
        "plt.figure(figsize=(15, 7)) # Adjust figure size for better readability\n",
        "pivot = topic_trend.pivot(index='month_str', columns='dominant_topic', values='count').fillna(0)\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
        "plt.title('Monthly Topic Trends')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Posts')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Dominant Topic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Histogram of likesCount distribution\n",
        "plt.figure(figsize=figsize)\n",
        "sns.histplot(df['likesCount'], bins=30, kde=True)\n",
        "plt.title('Distribution of Likes Count')\n",
        "plt.xlabel('Likes Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Scatter plot of likesCount vs text_len\n",
        "plt.figure(figsize=figsize)\n",
        "sns.scatterplot(data=df, x='text_len', y='likesCount', alpha=0.6)\n",
        "plt.title('Likes Count vs Text Length')\n",
        "plt.xlabel('Text Length (Number of words)')\n",
        "plt.ylabel('Likes Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88b3e566"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates an issue with the data type of the 'month' column when plotting. Although it's a Period object, seaborn might be having trouble with it. Convert 'month' to a string representation for plotting the time series data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-OvgHsq3tq5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert 'month' to string for plotting compatibility\n",
        "sent_trend['month_str'] = sent_trend['month'].astype(str)\n",
        "topic_trend['month_str'] = topic_trend['month'].astype(str)\n",
        "\n",
        "# 1. Line plot of monthly sentiment trend\n",
        "plt.figure(figsize=figsize)\n",
        "sns.lineplot(data=sent_trend, x='month_str', y='v_compound')\n",
        "plt.title('Monthly Sentiment Trend (Compound Score)')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Compound Sentiment Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Stacked bar chart of monthly topic trends\n",
        "# Convert 'dominant_topic' to string to ensure it's treated as categorical for stacking\n",
        "topic_trend['dominant_topic'] = topic_trend['dominant_topic'].astype(str)\n",
        "plt.figure(figsize=(15, 7)) # Adjust figure size for better readability\n",
        "pivot = topic_trend.pivot(index='month_str', columns='dominant_topic', values='count').fillna(0)\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
        "plt.title('Monthly Topic Trends')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Posts')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Dominant Topic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Histogram of likesCount distribution\n",
        "plt.figure(figsize=figsize)\n",
        "sns.histplot(df['likesCount'], bins=30, kde=True)\n",
        "plt.title('Distribution of Likes Count')\n",
        "plt.xlabel('Likes Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Scatter plot of likesCount vs text_len\n",
        "plt.figure(figsize=figsize)\n",
        "sns.scatterplot(data=df, x='text_len', y='likesCount', alpha=0.6)\n",
        "plt.title('Likes Count vs Text Length')\n",
        "plt.xlabel('Text Length (Number of words)')\n",
        "plt.ylabel('Likes Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2047611"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset contains 522 entries with columns for date, likesCount, profileName, commentsCount, and text.\n",
        "*   There are 278 missing values in `commentsCount` and 18 missing values in `text`.\n",
        "*   After preprocessing, including cleaning, tokenization, stop word removal, and lemmatization, a `clean_text` column was created for analysis.\n",
        "*   VADER sentiment analysis was applied, resulting in the addition of `v_neg`, `v_neu`, `v_pos`, `v_compound` scores, and a `vader_label` (positive, neutral, negative). The sentiment distribution shows a balance between neutral, positive, and negative labels.\n",
        "*   Topic modeling using LDA identified 6 dominant topics with associated keywords.\n",
        "*   Monthly trends for average compound sentiment and the distribution of dominant topics were calculated and visualized.\n",
        "*   A Linear Regression model was built to predict `likesCount` using `text_len`, `v_compound`, and topic probabilities as features.\n",
        "*   The Linear Regression model performed poorly in predicting `likesCount`, as indicated by an R-squared of -0.02 on the testing data.\n",
        "*   The distribution of `likesCount` is heavily skewed towards lower values.\n",
        "*   A scatter plot of `likesCount` versus `text_len` shows no clear linear relationship.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Given the poor performance of the initial Linear Regression model, explore alternative regression models (e.g., tree-based models, support vector machines) and potentially engineer more relevant features to improve the prediction of engagement metrics.\n",
        "*   Further investigate the relationship between text features (sentiment, topics, length) and engagement metrics beyond linear models to uncover potential non-linear patterns or interactions.\n"
      ]
    }
  ]
}